{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14703f7",
   "metadata": {},
   "source": [
    "## Training a BERT Model From Scratch\n",
    "- There are a few steps to the process, let’s first summarize what we need to do. In total, there are four key parts:\n",
    "    - Getting the data\n",
    "    - Building a tokenizer\n",
    "    - Creating an input pipeline\n",
    "    - Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4cad8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from transformers import RobertaTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08566f7",
   "metadata": {},
   "source": [
    "### Getting The Data\n",
    "- The data provided by the stakeholder is a Shakespeare Sonnet which we will use as our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d47a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('bert_unbiased.txt', 'r')\n",
    "\n",
    "\n",
    "lines = []\n",
    "with open('bert_unbiased.txt',  encoding=\"ISO-8859-1\") as t:\n",
    "    lines = t.readlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd81b4e",
   "metadata": {},
   "source": [
    "#### Text File\n",
    "- We have to store the data in a format that we can use when building the tokenizer. \n",
    "- We need to create a set of plaintext file containing just the text feature from our dataset, and we will split each sample using a newline \\n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a6952a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3951c5646690434a9e98bc714712f162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(lines):\n",
    "    sample = sample.replace('\\n', '')\n",
    "    text_data.append(sample)\n",
    "    if len(text_data) == 10_000:\n",
    "        # once we get the 10K mark, save to file\n",
    "        with open(f'/Users/gurneetbedi/Desktop/Capstone/Bert/Bert_Unbiased/Data/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "# after saving in 10K chunks, we will have ~2082 leftover samples, we save those now too\n",
    "with open(f'/Users/gurneetbedi/Desktop/Capstone/Bert/Bert_Unbiased/Data/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7b06f",
   "metadata": {},
   "source": [
    "### Building a tokenizer\n",
    "- We’ll start with a single sample and work through the preparation logic.\n",
    "\n",
    "- First, we will open the training data file — the same file that we saved as .txt files earlier. We split each based on newline characters \\n as this indicates the individual samples.\n",
    "\n",
    "- Next we will make a tokenizer, when using transformers we typically load a tokenizer, alongside its respective transformer model — the tokenizer is a key component in the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "982f5ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data/text_0.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = [str(x) for x in Path('./Data').glob('*.txt')]\n",
    "\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a4a4463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intiallizing the tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ef66b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(files=path, vocab_size = 30_522, min_frequency = 2,\n",
    "               special_tokens= ['<s>', '<pad>', '</s>', \"<unk>\", '<mask>'])\n",
    "\n",
    "# <s> = Start Sequence Token\n",
    "# <pad> = Padding Token\n",
    "# </s> = End of sequence\n",
    "# <unk> = Unkown Token\n",
    "# <mask>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "772504fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making directory to store the json and text file.\n",
    "\n",
    "os.mkdir('shakespeare_unbiased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64282e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model to the directory\n",
    "# These are the tokenization for our tokenizer.\n",
    "tokenizer.save_model('shakespeare_unbiased')\n",
    "\n",
    "# Two steps of tokenization\n",
    "# When we first feed data it goes into merges.txt they get translated into tokens,\n",
    "# Vocab.json token and token ids\n",
    "# merges.txt — performs the initial mapping of text to tokens\n",
    "# vocab.json — maps the tokens to token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d99913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer using the tokenizer we initialized and saved to file\n",
    "tokenizer = RobertaTokenizer.from_pretrained('shakespeare_unbiased', max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff5e1502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Output for token.\n",
    "# Once the tokenizer is ready, we can try encoding some text with it. \n",
    "# When encoding we will use the same two methods we would typically use, encode and encode_batch.\n",
    "tokens = tokenizer('fairest', padding = 'max_length', max_length = 12,return_tensors ='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bdd3901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,  74,  69, 473, 295,   2,   1,   1,   1,   1,   1,   1]])\n"
     ]
    }
   ],
   "source": [
    "# From the encodings object tokens we will be extracting the input_ids and\n",
    "# attention_mask tensors for use with shakespeare_unbiased.\n",
    "print(tokens.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4970986e",
   "metadata": {},
   "source": [
    "### Creating the Input Pipeline\n",
    "- Now we will move onto creating our tensors — we will be training our model through masked-language modeling (MLM). So, we need three tensors:\n",
    "    - input_ids — our token_ids with ~15% of tokens masked using the mask token <mask. \n",
    "    - attention_mask — a tensor of 1s and 0s, marking the position of ‘real’ tokens/padding tokens — used in attention calculations.\n",
    "    - labels — our token_ids with no masking.\n",
    "\n",
    "- Our attention_mask and labels tensors are simply extracted from our batch. The input_ids tensors require more attention however, for this tensor we mask ~15% of the tokens — assigning them the token ID 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75990bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm(tensor):\n",
    "    rand = torch.rand(tensor.shape) #[0, 1]\n",
    "    mask_arr = (rand < 0.15) * (tensor > 2) #0,1,2\n",
    "    for i in range(tensor.shape[0]):\n",
    "        selection = torch.flatten(mask_arr[i].nonzero()).tolist() #[[2,5,18]]\n",
    "        tensor[i, selection] = 4\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "356f91e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data/text_0.txt']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = [str(x) for x in Path('./Data').glob('*.txt')]\n",
    "\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b533ff33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bbc883a8fe429590f1066a8eac6365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating a tesnsor and appending values into each list.\n",
    "input_ids = []\n",
    "mask = []\n",
    "labels = []\n",
    "\n",
    "for path in tqdm(paths):\n",
    "    with open(path, 'r', encoding= 'utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "    sample = tokenizer(lines, max_length = 512, padding = 'max_length', truncation = True,return_tensors ='pt')\n",
    "    labels.append(sample.input_ids)\n",
    "    mask.append(sample.attention_mask)\n",
    "    input_ids.append(mlm(sample.input_ids.detach().clone()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ab4b6c",
   "metadata": {},
   "source": [
    "### Building the DataLoader\n",
    "- Next, we define our Dataset class — which we use to initialize our three encoded tensors as PyTorch torch.utils.data.Dataset objects.\n",
    "- Finally, our dataset is loaded into a PyTorch DataLoader object — which we use to load our data into our model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c91ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list with the specific length\n",
    "input_ids = torch.cat(input_ids)\n",
    "mask = torch.cat(mask)\n",
    "labels = torch.cat(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f407ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary with the input_ids, attention_mask and labels\n",
    "encodings = {\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask' : mask,\n",
    "    'labels' : labels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "520980b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataset object.\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __len__(self):\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return {key: tensor[i] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd3bdd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intiallizing the dataset class\n",
    "dataset = Dataset(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64aa5a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intiallizing the data loader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 16, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a5f14b",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "- We need two things for training, our DataLoader and a model. The DataLoader we have — but no model.\n",
    "    - Initializing the Model\n",
    "        - For training, we need a raw (not pre-trained) BERTLMHeadModel. To create that, we first need to create a RoBERTa config object to describe the parameters we’d like to initialize FiliBERTo with.\n",
    "        - Then, we import and initialize our RoBERTa model with a language modeling (LM) head.\n",
    "    - Training Preparation\n",
    "        - Before moving onto our training loop we need to set up a few things. First, we set up GPU/CPU usage. Then we activate the training mode of our model — and finally, initialize our optimizer.\n",
    "    - Training, after this we will train the PYtorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86b204af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert_unbiased.txt']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get a list of paths to each file in the directory\n",
    "\n",
    "paths_new = [str(x) for x in Path('./').glob('*.txt')]\n",
    "\n",
    "paths_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83d2605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config File\n",
    "config = RobertaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,  # we align this to the tokenizer vocab_size\n",
    "    max_position_embeddings=514,\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02808885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model\n",
    "model = RobertaForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af7c306b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(3040, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=3040, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup GPU/CPU usage.\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62edf18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate training mode\n",
    "model.train()\n",
    "# Initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c841dbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ccd155fe4d468eb86cd45f8ee92e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70afd7f0b6584febb599b093f564cd03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We train just as we usually would when training via PyTorch.\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "953428bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model.\n",
    "model.save_pretrained('shakespeare_unbiased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941b5631",
   "metadata": {},
   "source": [
    "### Testing the Model\n",
    "- Testing the output given by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "047ea87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill = pipeline('fill-mask', model = 'shakespeare_unbiased', tokenizer = 'shakespeare_unbiased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8073ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '? the world, or else this glutton be',\n",
       "  'score': 0.166957288980484,\n",
       "  'token': 35,\n",
       "  'token_str': '?'},\n",
       " {'sequence': 'and the world, or else this glutton be',\n",
       "  'score': 0.16617439687252045,\n",
       "  'token': 304,\n",
       "  'token_str': 'and'},\n",
       " {'sequence': 'the the world, or else this glutton be',\n",
       "  'score': 0.01064683310687542,\n",
       "  'token': 377,\n",
       "  'token_str': 'the'},\n",
       " {'sequence': 'to the world, or else this glutton be',\n",
       "  'score': 0.008266780525445938,\n",
       "  'token': 385,\n",
       "  'token_str': 'to'},\n",
       " {'sequence': 'but the world, or else this glutton be',\n",
       "  'score': 0.006061549764126539,\n",
       "  'token': 389,\n",
       "  'token_str': 'but'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill(f'{fill.tokenizer.mask_token} the world, or else this glutton be')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6af72c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'Learning, the teacher',\n",
       "  'score': 0.07089152932167053,\n",
       "  'token': 16,\n",
       "  'token_str': ','},\n",
       " {'sequence': 'Learning my the teacher',\n",
       "  'score': 0.023096293210983276,\n",
       "  'token': 298,\n",
       "  'token_str': ' my'},\n",
       " {'sequence': 'Learning I the teacher',\n",
       "  'score': 0.017750972881913185,\n",
       "  'token': 303,\n",
       "  'token_str': ' I'},\n",
       " {'sequence': 'Learning and the teacher',\n",
       "  'score': 0.017463266849517822,\n",
       "  'token': 318,\n",
       "  'token_str': ' and'},\n",
       " {'sequence': 'Learning of the teacher',\n",
       "  'score': 0.016983861103653908,\n",
       "  'token': 296,\n",
       "  'token_str': ' of'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill(f'Learning {fill.tokenizer.mask_token} the teacher')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
